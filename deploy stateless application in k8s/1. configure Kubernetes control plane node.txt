ssh student@ca-lab-vm
ssh student@40.124.54.232

configuring a Red Hat Enterprise Linux 7 VM as a Kubernetes control plane node

The control plane node in Kubernetes provides the cluster's control plane. Thecontrol plane node makes global decisions for the cluster, such as scheduling and taking action to meet service replication requirements.

Thecontrol plane node is made up of several components:

kube-apiserver: Exposes the Kubernetes API
etcd: Durable, consistent, persistent storage of Kubernetes objects that describe the state of your cluster
kube-controller-manager: Runs controllers responsible for routine cluster tasks, such as detecting when nodes go down
kube-scheduler: Selects worker nodes to assign pods to based on resource requirements and other constraints. Pods are logically grouped containers.
Addons: Provide additional cluster features, such as DNS, and user interface

This Lab Step takes you through the process of configuring a Red Hat Enterprise Linux 7 VM as a Kubernetes control plane node. There are several ways to achieve this goal. For instructional purposes, you will use the yum package manager and manually modify necessary configuration files. This approach will show you everything that is required to set up a control plane node while avoiding many pitfalls. In practice, you can automate this method using configuration management tools or deploy the control plane as a Docker container. You can also use images and scripts provided by a cloud provider to get up and running quickly in the cloud.



$ sudo yum install -y kubernetes


# Dependency Installed:
  kubernetes-client.x86_64 0:1.5.2-0.7.git269f928.el7                           
  kubernetes-master.x86_64 0:1.5.2-0.7.git269f928.el7                           
  kubernetes-node.x86_64 0:1.5.2-0.7.git269f928.el7                             



$ sudo yum install -y etcd


# config files for kubernetes components
$ ls /etc/kubernetes


# In the common config file, change the KUBE_MASTER API server address to use the control plane's DNS name

$ sudo sed -i 's/\(KUBE_MASTER.*\)127.0.0.1\(.*\)/\1ca-lab-vm\2/g' /etc/kubernetes/config

# This command replaces the default control plane host of 127.0.0.1 with ca-lab-vm. Within the virtual network, the server can be addressed by ca-lab-vm. In the command, the \1 and \2 before and after ca-lab-vm expand to everything that is before and after 127.0.0.1 in the original line. This has the effect of only changing 127.0.0.1 to ca-lab-vm in the line. 



# In the apiserver configuration file, change the bind address to listen on all interfaces (0.0.0.0):

$ sudo sed -i 's/\(KUBE_API_ADDRESS.*\)127.0.0.1\(.*\)/\10.0.0.0\2/g' /etc/kubernetes/apiserver

# This allows other nodes to communicate with the Kubernetes API.



# Create a key to authorize API requests from pods

$ openssl genrsa -out /tmp/serviceaccount.key 2048

# For convenience, it is stored in /tmp. In a production environment, you would protect the key better.

# A specific controller called the admissions controller is responsible for authorizing API server requests. A service account is used to identify processes that run in pods. A service account called default is automatically created for this purpose. The key you created will be used to authorize the default service account requests.



# Specify the service account key to use for the API server and the controller manager

$ sudo sed -i 's/\(KUBE_API_ARGS=\).*/\1"--service_account_key_file=\/tmp\/serviceaccount.key"/' /etc/kubernetes/apiserver
sudo sed -i 's/\(KUBE_CONTROLLER_MANAGER_ARGS=\).*/\1"--service_account_private_key_file=\/tmp\/serviceaccount.key"/' /etc/kubernetes/controller-manager



# Enter the following to configure etcd to also listen on all interfaces:

$ sudo sed -i 's/\(ETCD_LISTEN_CLIENT_URLS.*\)localhost\(.*\)/\10.0.0.0\2/g' /etc/etcd/etcd.conf

# Start, enable the services on reboot, and list the status of the services required by the control plane


$ for SERVICE in etcd kube-apiserver kube-controller-manager kube-scheduler; do
    sudo systemctl restart $SERVICE
    sudo systemctl enable $SERVICE
    sudo systemctl status $SERVICE
done



● etcd.service - Etcd Server
   Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2022-06-07 17:23:13 UTC; 182ms ago
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2022-06-07 17:23:13 UTC; 194ms ago
● kube-controller-manager.service - Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2022-06-07 17:23:14 UTC; 223ms ago
● kube-scheduler.service - Kubernetes Scheduler Plugin
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2022-06-07 17:23:14 UTC; 210ms ago



# Verify the Kubernetes API server by issuing an HTTP GET request to return all the API groups supported by the server:

$ curl http://ca-lab-vm:8080/apis | more

# Enter the following command to show the versions of supported API groups:

$ kubectl api-versions


apps/v1beta1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1beta1
autoscaling/v1
batch/v1
certificates.k8s.io/v1alpha1
extensions/v1beta1
policy/v1beta1
rbac.authorization.k8s.io/v1alpha1
storage.k8s.io/v1beta1
v1

# Enter the following command to re-run the last command with verbose output

$ kubectl api-versions --v=7





# Create a YAML file describing a worker node resource

cat <<EOF > node.yml
apiVersion: v1
kind: Node
metadata:
  name: k8s-node
  labels:
    # Use labels to organize nodes
    environment: dev
spec:
  # Fully qualified domain name to reach the node (metadata.name by default)
  externalID: k8s-node
EOF


# This node object describes a node and how to communicate with the node. The next Lab Step will actually configure the worker node.

# Note that the worker node has an externalID of k8s-node.

# This name is resolvable by the internal DNS to the private IP of the worker node. This happens automatically because the virtual machine is named k8s-node. You can verify this by entering host k8s-node.


$ host k8s-node
k8s-node.2irhxuoz2rkefcnqaz1engmgzd.jx.internal.cloudapp.net has address 10.0.0.10


# Create the worker node resource

$ kubectl create -f node.yml

# Enter the following command to display all the node resources in the cluster along with their labels

$ kubectl get nodes --show-labels
NAME       STATUS    AGE       LABELS
k8s-node   Unknown   28s       environment=dev

NAME       STATUS     AGE       LABELS
k8s-node   NotReady   3m        environment=dev


# You will configure the worker node in the next Lab Step to make its STATUS Ready.

# In order for the worker node to be able to connect to the kube-apiserver on the control plane node, you need to open allow the connection:

$ sudo systemctl stop firewalld

# This stops the operating system's firewall all together. For the purpose of the Lab, this is acceptable. The Azure network security group is still protecting the VM from external networks. However, in production you would want to create firewall rules that only allow required traffic.